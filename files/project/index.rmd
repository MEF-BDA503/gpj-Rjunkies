---
title: "R_Junkies Group Project"
output:
  html_document:
    toc: true
    theme: cosmo
    highlight: haddock
    toc_float:
      collapsed: true
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction {#intro}

This project, by **R_Junkies**, is about past airplane crashes, from 1908 to 2017. The project is prepared specifically for BDA 503, 2017, MEF University; course given by [Berk Orbay](http://berkorbay.me/).

## About Group Members

We call ourselves: **R_Junkies**. It represents our humble interest in Data Science. It's a life-style.

Group members are below (Ladies first):

+ [Yağmur Ulutürk Tekten](https://tr.linkedin.com/in/yagmuruluturk)
+ [Cem Gürkan](https://tr.linkedin.com/in/cgurkan)
+ [Rezan Azizoğlu](https://tr.linkedin.com/in/umut-rezan-azizoglu-b6683146)
+ [Semih Tekten](https://tr.linkedin.com/in/semihtekten)

## About R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

# Gathering Data {#gather}

Here is the list of datasets and sources we will be using in this project:

+ Airplane Crashes Since 1908 - Kaggle [Link](https://www.kaggle.com/saurograndi/airplane-crashes-since-1908)
+ Air transport, passengers carried (1970-2016) - World Bank [Link](http://databank.worldbank.org/data/reports.aspx?source=2&series=IS.AIR.PSGR&country=)  
+ Air transport, registered carrier departures worldwide (1970-2016) - World Bank [Link](http://databank.worldbank.org/data/reports.aspx?source=2&series=IS.AIR.DPRT&country=) 

Our dataset taken from Kaggle contains only data year from 1908 to 2009 (partialy). In order to analyze data more accurately, we crawl dataset from [http://www.planecrashinfo.com/](http://www.planecrashinfo.com/). With the following Pyhton code, we create a new file contains only data from 2009 to 2017.

```{python}
from bs4 import BeautifulSoup
import re
from urllib import request
import datetime
import csv

# Request URL and return response as BeautifulSoup object
def makeBeautifulSoupObject(url):
    requestConn = request.urlopen(url)
    responseHTML = requestConn.read()
    requestConn.close()
    soup = BeautifulSoup(responseHTML, "lxml")
    soup.decode(eventual_encoding="UTF-8")
    return soup

def parseHTML(table_):
    record = {}
    table = BeautifulSoup(str(table_[0]), 'html.parser')

    for tr in table.find_all("tr")[1:]:
        tds = tr.find_all("td")
        # encoding the 'value' string to utf-8 and removing any non-breaking space (HTML Character)
        tmp_str = tds[1].string  # .string.encode('utf-8').replace("&nbsp;", "")
        value = str(tmp_str)  # this is the value- In Column #2 of the HTML table
        key = tds[0].string  # this is the key- In Column #1 of the HTML table
        # print(tds[0])
        if key == "Date:":
            dat = str(value).replace(',', '')
            date = datetime.datetime.strptime(dat, '%B %d %Y')
            record["date"] = date
        elif key == "Time:":
            if not value == '?':
                time = re.sub("[^0-9]", "", value)
                record["time"] = time[0:2] + ":" + time[2:4]
            else:
                record["time"] = ''
        elif key == "Location:":
            if not value == '?':
                record["loc"] = str(value)
            else:
                record["loc"] = ''
        elif key == "Operator:":
            if not value == '?':
                record["op"] = str(value)
            else:
                record["op"] = ''
        elif key == "Flight #:":
            if not value == '?':
                record["flight"] = str(value)
            else:
                record["flight"] = ''
        elif key == "Route:":
            if not value == '?':
                record["route"] = str(value)
            else:
                record["route"] = ''
        elif key == "Registration:":
            if not value == '?':
                record["reg"] = str(value).encode("utf-8")
            else:
                record["reg"] = ''
        elif key == "cn / ln:":
            if not value == '?':
                record["cnln"] = str(value)
            else:
                record["cnln"] = ''
        elif key == "Aboard:":
            if not value == '?':
                s = ' '.join(value.split())
                aboard_ = s.replace('(', '').replace(')', '').split(' ')
                if aboard_[0] != '?':
                    record["aboard_total"] = aboard_[0]
                else:
                    record["aboard_total"] = 'NULL'

                passengers = aboard_[1].replace("passengers:", "")
                if passengers != '?':
                    record["aboard_passengers"] = passengers
                else:
                    record["aboard_passengers"] = 'NULL'

                crew = aboard_[2].replace("crew:", "")
                if crew != '?':
                    record["aboard_crew"] = crew
                else:
                    record["aboard_crew"] = 'NULL'
            else:
                record["aboard_total"] = 'NULL'
                record["aboard_passengers"] = 'NULL'
                record["aboard_crew"] = 'NULL'
        elif key == "Fatalities:":
            if not value == '?':
                s = ' '.join(value.split())
                fatalities_ = s.replace('(', '').replace(')', '').split(' ')

                if fatalities_[0] != '?':
                    record["fatalities_total"] = fatalities_[0]
                else:
                    record["fatalities_total"] = 'NULL'

                passengers = fatalities_[1].replace("passengers:", "")
                if passengers != '?':
                    record["fatalities_passengers"] = passengers
                else:
                    record["fatalities_passengers"] = 'NULL'

                crew = fatalities_[2].replace("crew:", "")
                if crew != '?':
                    record["fatalities_crew"] = crew
                else:
                    record["fatalities_crew"] = 'NULL'
            else:
                record["aboard_total"] = 'NULL'
                record["aboard_passengers"] = 'NULL'
                record["aboard_crew"] = 'NULL'
        elif key == "Ground:":
            if not value == '?':
                record["ground"] = str(value)
            else:
                record["ground"] = 'NULL'
        elif key == "Summary:":
            if not value == '?':
                record["summary"] = str(value).replace('\n','')
            else:
                record["summary"] = ''
        elif key == "AC Type:":
            if not value == '?':
                record["actype"] = str(value)
            else:
                record["actype"] = ''
        else:
            st1 = ''.join(tds[0].string.split()).lower()
            st1 = st1.replace(':', '')
            if not value == '?':
                record[st1] = str(value)
            else:
                record[st1] = "NULL"
    return record

### Main Program ###

# Root URL
rooturl = "http://www.planecrashinfo.com"
# From which year data is taken
start_year = 2009
# to which year data is taken
end_year = 2009

# Create a new file
f = csv.writer(open("Airplane_Crashes_and_Fatalities_Since_2009.csv", "w", newline=''), delimiter=',')
# Header info
f.writerow(["Date", "Time", "Location", "Operator", "Flight #", "Route", "Type", "Registration", "cn/In", "Aboard", "Fatalities", "Ground", "Summary"])

for i in range(start_year, end_year + 1, 1):
    year_start = datetime.datetime.utcnow()
    # appending the path (year) to the url hostname
    # Sample page : http://www.planecrashinfo.com/2008/2008.htm
    newurl = rooturl + "/" + str(i) + "/" + str(i) + ".htm"
    #Get the main page for each year
    soup = makeBeautifulSoupObject(newurl)
    tables = soup.find_all('table')

    #Each page contains the sumamry crash info
    for table in tables:
        # finding the no. of records for the given year
        number_of_rows = len(table.findAll(lambda tag: tag.name == 'tr' and tag.findParent('table') == table))
        number_of_rows = 3

        for j in range(1, number_of_rows, 1):
            # appending the row number to sub-path of the url, and building the final url that will be used for sending http request
            #Sample : http://www.planecrashinfo.com/2008/2008-1.htm
            accident_url = newurl.replace(".htm", "") + "-" + str(j) + ".htm"
            web_record = makeBeautifulSoupObject(accident_url)
            # removing all the boilerplate html code except the data table
            table_details = web_record.find_all('table')
            #Parse table and return crash record
            crashRecord = parseHTML(table_details)
            #Write crash record to file
            f.writerow([crashRecord["date"].strftime('%m/%d/%Y'),
                        crashRecord["time"],
                        crashRecord["loc"],
                        crashRecord["op"],
                        crashRecord["flight"],
                        crashRecord["route"],
                        crashRecord["actype"],
                        crashRecord["reg"],
                        crashRecord["cnln"],
                        crashRecord["aboard_total"],
                        # record["aboard_passengers"],
                        # record["aboard_crew"],
                        crashRecord["fatalities_total"],
                        # record["fatalities_passengers"],
                        # record["fatalities_crew"],
                        crashRecord["ground"],
                        crashRecord["summary"]
                        ])
```


## Airplane Crashes

```{r include=TRUE}

#Original Kaggle file
apc_raw_to_2009 <- read.csv(file="https://raw.githubusercontent.com/MEF-BDA503/gpj-rjunkies/master/files/project_data/Airplane_Crashes_and_Fatalities_Since_1908.csv", header=TRUE, sep=",")

#Data we crawl from web
apc_raw_from_2009 <- read.csv(file="https://raw.githubusercontent.com/MEF-BDA503/gpj-rjunkies/master/files/project_data/Airplane_Crashes_and_Fatalities_Since_2009.csv", header=TRUE, sep=",")

```

## Passengers Carried

```{r include=TRUE}

air_psgr <- read.csv(file="https://raw.githubusercontent.com/MEF-BDA503/gpj-rjunkies/master/files/project_data/is_air_psgr_melt.csv", header=TRUE, sep=",", quote="\"", na.strings="NA")

```


## Registered Carrier Departures Worldwide

```{r include=TRUE}

air_dprt <- read.csv(file="https://raw.githubusercontent.com/MEF-BDA503/gpj-rjunkies/master/files/project_data/is_air_dprt_melt.csv",header=TRUE, sep=",", quote="\"", na.strings="NA")

```

<br>

# Data Preprocessing & Cleaning

Here are the necessary libraries. We checked if necessary libraries are not installed properly.

```{r echo=TRUE, warning=FALSE, message=FALSE}
necessary_lib <- c("stringr","tidyverse","ggthemes","grid","gridExtra","scales","tm","SnowballC","wordcloud","RColorBrewer")

# Lets check this necessary libraries against already installed packages.
necessary_lib <- necessary_lib[!(necessary_lib %in% installed.packages())]

if(length(necessary_lib)!=0){
  install.packages(necessary_lib)
}

```

Load necessary libraries


```{r echo=TRUE, warning=FALSE, message=FALSE}

library(stringr) # For string manipulation
library(tidyverse)
library(ggthemes)
library(grid)
library(gridExtra)
library(scales)

library(tm) # for text mining
library(SnowballC) # for text stemming
library(wordcloud) # word-cloud generator 
library(RColorBrewer) # color palettes

```

In the next section, we merge the dataset from Kaggle and dataset we crawl in to one dataframe.
Kaggle dataset contains partial year 2009 data so we omit year 2009 data.

```{r include=TRUE}
#Clean data greater than year 2009 data from orginal kaggle file due to amended file contains all 2009 crashes
apc_raw_to_2009 <-  apc_raw_to_2009 %>%
                    mutate(year=format(as.Date(apc_raw_to_2009$Date, format="%m/%d/%Y"),"%Y")) %>%
                    filter(as.numeric(year) < 2009) %>%
                    select(-year)

#Binding data
apc_raw <- rbind(apc_raw_to_2009, apc_raw_from_2009)

```


We first create a new data frame called "apc_clean" which we will manipulate for future analysis.

```{r include=TRUE}

# New data frame
apc_clean <- apc_raw

# Dimensions
dim(apc_clean)

# Get an idea of apc_clean columns
str(apc_clean)

```

There are columns which we can't use for any analysis. These are: "**Flight..**", "**Registration**", "**cn.In**", "**Ground**"

```{r include=TRUE}

# Delete "FLight.." column
apc_clean$Flight.. <- NULL

# Delete "Registration" column
apc_clean$Registration <- NULL

# Delete "cn.In" column
apc_clean$cn.In <- NULL

# Delete "Ground" column
apc_clean$Ground <- NULL

```

Beautiful. We dropped unnecessary columns.<br>
Now it's time to reshape time-related columns.

```{r include=TRUE}

# Change date format
apc_clean$Date <- as.Date(apc_clean$Date, format = "%m/%d/%Y")
typeof(apc_clean$Date)

# Change & clean time format
apc_clean$Time <- gsub('c:', '', apc_clean$Time)
apc_clean$Time <- gsub('c', '', apc_clean$Time)
apc_clean$Time <- factor(as.character(substr(apc_clean$Time, 1, 2)))
typeof(apc_clean$Time)

# New columns for month & year
apc_clean$Year = factor(format(as.Date(apc_clean$Date, format="%Y/%m/%d"),"%Y"))
apc_clean$Month = factor(format(as.Date(apc_clean$Date, format="%Y/%m/%d"),"%m"))

head(apc_clean$Year)
head(apc_clean$Month)

```

We can collect state & city information from "Location" column.

```{r include=TRUE}

# We add new "State" & "City" column
apc_clean$State <- sapply(apc_clean$Location, as.character)
apc_clean$City <- sapply(apc_clean$Location, as.character)

# Seperate State & City
apc_clean$State <- str_trim(gsub(".*,", "", apc_clean$State))
apc_clean$City <- str_trim(gsub(",.*", "", apc_clean$City))

head(apc_clean$State)
head(apc_clean$City)

# Delete unnecessary "Location" column
apc_clean$Location <- NULL

```

Very nice. Now we are going to label flights as "Civilian" or "Military". If "IsMilitary" equals to 1, that means that flight operator is a Military institution, otherwise it is a Civilian flight.

```{r include=TRUE}

military_keywords <- c("Military", "Army", "Navy")
apc_clean$IsMilitary <- ifelse(grepl(paste(military_keywords, collapse = "|"), apc_clean$Operator),1,0)

# Number of Military Flights
sum(apc_clean$IsMilitary)

```

Wonderful. Let's find out how many passengers survived for each crash.

```{r include=TRUE}

apc_clean$Survived <- apc_clean$Aboard - apc_clean$Fatalities
head(apc_clean$Survived)

```

We can determine Source, Destination and number of stops from "Route" column.

```{r include=TRUE}

apc_clean$Source <- gsub(" -.*", "", apc_clean$Route)
apc_clean$Destination <- gsub(".* -", "", apc_clean$Route)
apc_clean$Stops <- str_count(apc_clean$Route,"-")-1

# Number of flights with no Stops
length(which(apc_clean$Stops==0))

# Number of flights with no Route information
length(which(apc_clean$Stops<0))

# Number of flights with Stops
length(which(apc_clean$Stops>0))

```

Lastly, we reordered the columns in apc_clean data frame.

```{r include=TRUE}

apc_clean <- apc_clean[,c(1,2,9,10,11,12,3,13,4,15,16,17,5,6,7,14,8)]

# Latest structure of apc_clean
str(apc_clean)

```


You can find references for preprocessing from [here](#ref_preprocessing).
<br><br>

# Exploratory Analytics of APC {#eda}

## Explaining the Dataset & Variables

**Dataset**:
Airplane crashes from 1908 to 2017. Contains 17 variables, with 5.543 observations.

Here are the variables in our Airplane Crashes dataset and their explanations:

+ **Date**: The date of airplane crash.
+ **Time**: The time of airplane crash in hh format.
+ **Year**: The year of airplane crash.
+ **Month**: The month of airplane crash.
+ **State**: This field shows the country in which airplane crash happened.
+ **City**: This field shows the city in which airplane crash occured.
+ **Operator**: This field contains the airline information.
+ **IsMilitary**: This field shows whether an operator is a military institution or civilian. It conveys binary results (1=Military, 0=Civilian).
+ **Route**: This field contains the  departure, arrival and transfer location of the flight. There are both direct and connecting flights.
+ **Source**: This field shows the city where the airplane took-off.
+ **Destination**: This field shows the destination city.
+ **Stops**: This field demonstrates the number of transfer cities for connecting flights.
+ **Type**: This field contains the information of the airplane type.
+ **Aboard**: This field shows the number of passengers at the time of departure.
+ **Fatalities**: This field shows the number of passengers died after airplane crash.
+ **Survived**: This field shows the number of passengers survived after airplane crash.
+ **Summary**: This field contains information about the airplane crash and possible reasons.

## Objectives

+ Understanding statistics and characteristics of past airplane crashes

+ Visualizing results of exploratory analysis

+ Demonstrating geolocations of airplane crashes on a dynamic map (Using Shiny)

+ Forecasting least risky flights for our future travel plans :)

+ Developing and demonstrating R, RMarkdown & Shiny skills of **R_Junkies**

## Questions

We studied most of the kernels on [Kaggle](https://www.kaggle.com/saurograndi/airplane-crashes-since-1908/kernels). Majority of the them are not very sophisticated and don't even go beyond basic descriptive analytics. We hope that at the end of our research, we can share our findings on Kaggle. :) We showed the questions with an asterix (\*) that we got inspired from kernels. Questions without an asterix are developed specifically for this analysis by **R_Junkies**.
<br><br>

todo: QUESTIONS WILL BE HERE.

## EDA

### Top 10 Airplane Types & Crashes

```{r include=TRUE}

top_ten_plane <- apc_clean %>%
                  group_by(Type) %>%
                  summarise(Freq = n()) %>%
                  arrange(desc(Freq)) %>% 
                  top_n(10)

top_ten_plane$Type <- substr(top_ten_plane$Type,1,19)

top_ten_plane


```


```{r include=TRUE}
top_ten_mil_plane <- apc_clean %>%
                  filter(IsMilitary==1) %>%
                  group_by(Type) %>%
                  summarise(Freq = n()) %>%
                  arrange(desc(Freq)) %>% 
                  top_n(10)

# Slicing long strings
top_ten_mil_plane$Type <- substr(top_ten_mil_plane$Type,1,19)

top_ten_mil_plane

top_ten_civ_plane <- apc_clean %>%
                  filter(IsMilitary==0) %>%
                  group_by(Type) %>%
                  summarise(Freq = n()) %>%
                  arrange(desc(Freq)) %>% 
                  top_n(10)

# Slicing long strings
top_ten_civ_plane$Type <- substr(top_ten_civ_plane$Type,1,19)

top_ten_civ_plane



```


```{r include=TRUE}

ggplot(top_ten_plane, aes(x=reorder(Type, -Freq), y=Freq)) +
  geom_bar(stat = "identity", fill = "#2780E3") +
  labs(title="Top 10 Airplanes by Crash Count",x="Airplanes",y="Frequency",fill="") +
  theme (axis.text.x=element_text (angle=60,vjust=1, hjust=1))



```

```{r include=TRUE}

top_ten_mil_plane_plot <- ggplot(top_ten_mil_plane, aes(x=reorder(Type, -Freq), y=Freq)) +
                            geom_bar(stat = "identity", fill = "#2780E3") +
                            labs(title="Top 10 Military Airplanes by Crash Count",x="Airplanes",y="Frequency",fill="") +
                            theme (axis.text.x=element_text (angle=60,vjust=1, hjust=1)) +
                            theme(plot.title = element_text(size=11),
                                  axis.text=element_text(size=8))


top_ten_civ_plane_plot <- ggplot(top_ten_civ_plane, aes(x=reorder(Type, -Freq), y=Freq)) +
                            geom_bar(stat = "identity", fill = "#2780E3") +
                            labs(title="Top 10 Civilian Airplanes by Crash Count",x="Airplanes",y="Frequency",fill="") +
                            theme (axis.text.x=element_text (angle=60,vjust=1, hjust=1)) +
                            theme(plot.title = element_text(size=11),
                                  axis.text=element_text(size=8))

grid.arrange(top_ten_mil_plane_plot, top_ten_civ_plane_plot, ncol=2)

```


### Comparing # of Accidents & # of Departures

```{r include=TRUE}

# Total departures in Million

total_dprt <- 
  air_dprt %>%
  filter(Time >= 1970 & Time <= 2016) %>%
  mutate(dep_val = ifelse(is.na(Value), 0, Value)) %>% 
  group_by(Time) %>% summarize(total_dep=sum(dep_val)/1000000)

# # of Accidents and Fatalities

Acc_fat_data <- apc_clean %>% filter(as.numeric(as.character(Year)) >= 1970 & as.numeric(as.character(Year)) <= 2016) %>%  group_by(Year)%>% summarise(n=sum(ifelse(is.na(Aboard), 0, Aboard)),f=sum(ifelse(is.na(Fatalities), 0, Fatalities)))

# Combining Departures and Accident-Fatality Data

dep_acc_fat <- data.frame(year=total_dprt$Time,
                Departure = total_dprt$total_dep,
                Accident = Acc_fat_data$n,
                Fatality = Acc_fat_data$f,
                Rate = Acc_fat_data$n/total_dprt$total_dep)

# Normalizer for plot

normalizer <- max(dep_acc_fat$Accident)/max(dep_acc_fat$Departure)

ggplot(dep_acc_fat, aes(y=Departure, x=year)) +  
  geom_col(aes(y = Accident / normalizer), fill = "#2780E3") + 
  geom_line(size=1.5, alpha=1, colour = "#7a1c1c") +
  scale_y_continuous(sec.axis = sec_axis(trans= ~.*normalizer, name = 'Accident')) +
  theme(axis.text.y = element_text(colour="#2780E3", size=12), 
    axis.text.x=element_text(angle=60, vjust=1, hjust=1, size=8),
    axis.title=element_text(colour="#2780E3", size=12),
    legend.position = "left" ) +
  labs(y="Departures", x="Year")


```


### Top Civil Operators' Crashes & Fatalities Over Years

```{r include=TRUE}

top_six_civ_op_crash <- apc_clean %>%
                        filter(IsMilitary==0) %>%
                        group_by(Operator) %>%
                        summarise(SumCrashes= n()) %>%
                        arrange(desc(SumCrashes))  %>%
                        top_n(6)

top_six_civ_op_fat <- apc_clean %>%
                        filter(IsMilitary==0) %>%
                        group_by(Operator) %>%
                        summarise(SumFats= sum(Fatalities)) %>%
                        arrange(desc(SumFats))  %>%
                        top_n(6)

top_six_civ_op_crash_oy <- apc_clean %>%
                            filter(Operator %in% top_six_civ_op_crash$Operator)  %>%
                            group_by(Operator, Year) %>%
                            summarise(SumFatalities= sum(Fatalities)) %>%
                            arrange(desc(Year), desc(SumFatalities))

top_six_civ_op_fat_oy <- apc_clean %>%
                            filter(Operator %in% top_six_civ_op_fat$Operator)  %>%
                            group_by(Operator, Year) %>%
                            summarise(SumFatalities= sum(Fatalities)) %>%
                            arrange(desc(Year), desc(SumFatalities))

top_six_civ_op_crash 
top_six_civ_op_fat
top_six_civ_op_crash_oy
top_six_civ_op_fat_oy

```

```{r include=TRUE}

ggplot(top_six_civ_op_crash_oy, aes(x=Year)) +
  geom_line(aes(y=SumFatalities, group = top_six_civ_op_crash_oy$Operator, colour=top_six_civ_op_crash_oy$Operator), size=0.5, alpha=1) +
  ggtitle("Fatalities of Top 6 Operators with Most Crashes Over Years") +
  scale_x_discrete(breaks = levels(top_six_civ_op_crash_oy$Year)[c(T, rep(F, 3))]) +
  theme (axis.text.x=element_text (angle=60,vjust=1, hjust=1)) +
  theme(plot.title = element_text(size=11),
         axis.text=element_text(size=8),
        legend.position="bottom") +
  labs(x = "Years", y = "Fatalities", colour = "Operators")

ggplot(top_six_civ_op_fat_oy, aes(x=Year)) +
  geom_line(aes(y=SumFatalities, group = top_six_civ_op_fat_oy$Operator, colour=top_six_civ_op_fat_oy$Operator), size=0.5, alpha=1) +
  ggtitle("Fatalities of Top 6 Operators with Most Fatalities Over Years") +
  scale_x_discrete(breaks = levels(top_six_civ_op_fat_oy$Year)[c(T, rep(F, 3))]) +
  theme (axis.text.x=element_text (angle=60,vjust=1, hjust=1)) +
  theme(plot.title = element_text(size=11),
         axis.text=element_text(size=8),
        legend.position="bottom") +
  labs(x = "Years", y = "Fatalities", colour = "Operators")

```

### Has Survival Rate Increased Over Time?
 
```{r include=TRUE, warning=FALSE}

# Survival Rate
dataSurvival <- apc_clean %>% 
   filter(as.numeric(as.character(Year)) >= 1900 & Aboard > 0) %>%
   group_by(Year) %>%
   summarize(TotalFatalities=sum(Fatalities), 
             TotalAboard=sum(Aboard),
             SurvivalRate=round(100*(TotalAboard-TotalFatalities)/TotalAboard,2),
             Survival = (TotalAboard-TotalFatalities),
             TotalAccident = n())
 
# Normalizer for dual-y axis
n <- max(dataSurvival$TotalAccident) /max(dataSurvival$SurvivalRate)
 
ggplot(dataSurvival, aes(y=SurvivalRate, x=Year)) +  
   geom_col(aes(y=TotalAccident/n), fill = "#2780E3") +
   geom_line(aes(group = 1), size=0.7, alpha=1, colour = "#7a1c1c") +
   geom_smooth(aes(group = 1), colour = "black") +
   scale_y_continuous(sec.axis = sec_axis(trans= ~.*n, name = 'Total Accidents')) + 
   scale_x_discrete(breaks = levels(dataSurvival$Year)[c(T, rep(F, 3))]) +
   
   theme(axis.text.x=element_text(angle=60, vjust=1, hjust=1, size=8))
   
   
```
 
We observed that "Survival Rate" was very low in early 1900's and increased over time. As seen in graph, at World War II, survival rate decreases below 10%. After 2000s, it increases to 50% on average.


### Possible Causes of Crashes - Word Cloud

For each accident, there is an explanation in "Summary" column. By looking at this summary information, we tried to visuailize a word cloud.
 
```{r include=TRUE, warning=FALSE}
 
 text <- apc_clean$Summary
 
 docs <- Corpus(VectorSource(text))
 
 toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
 docs <- tm_map(docs, toSpace, "/")
 docs <- tm_map(docs, toSpace, "@")
 docs <- tm_map(docs, toSpace, "\\|")
 
 docs <- tm_map(docs, PlainTextDocument)
 
 # Convert text to lower case
 docs <- tm_map(docs, content_transformer(tolower))
 
 # Remove numbers
 docs <- tm_map(docs, removeNumbers)
 
 # Remove english common stopwords
 docs <- tm_map(docs, removeWords, stopwords("english"))
 
 # Remove punctuations
 docs <- tm_map(docs, removePunctuation)
 
 # Eliminate extra white spaces
 docs <- tm_map(docs, stripWhitespace)
 
 # Text stemming
 docs <- tm_map(docs, stemDocument)
 
 # Remove possible words that high frequency in text that no meaning for crash reason
 # (After first try in word cloud, we realize that these words are the most frequent one)
 docs <- tm_map(docs, removeWords, c("aircraft", "plane","flight", "crash","crashed")) 
 
 dtm <- TermDocumentMatrix(docs)
 
 ##Remove sparse terms
 removeSparseTerms(dtm, 0.95)
 m <- as.matrix(dtm)
 v <- sort(rowSums(m),decreasing=TRUE)
 d <- data.frame(word = names(v),freq=v)
 #head(d, 10)
 
 set.seed(1234)
 wordcloud(words = d$word, freq = d$freq, min.freq = 1,
           max.words=200, random.order=FALSE, rot.per=0.35, 
           colors=brewer.pal(8, "Dark2"))
```

# Top 10 Frequency of Words

```{r include=TRUE, warning=FALSE}

#Top 10 Word frequencies
ggplot(d[1:10,], aes(x=reorder(word, -freq), y=freq)) +
  geom_bar(stat = "identity", fill = "#2780E3") +
  labs(title="Top 10 Word frequencies",x="Words",y="Frequency",fill="") +
  theme (axis.text.x=element_text (angle=60,vjust=1, hjust=1))

#How words associated with others. so we can understand causes 
i <- 1

while (i < 6) {
  print (findAssocs(dtm, terms = as.character(d[i,]$word), corlimit = 0.3))
  i = i+1
}

```


<br><br>

# Dynamic Map of Airplane Crashes
todo: SHINY
<br><br>

# Forecasting & Machine Learning
todo: PCA, Modelling etc.
<br><br>

# Conclusion

todo: Findings & insights for future researches
<br><br>

<br><br>

# References {#references}
## Data Preprocessing & Cleaning {#ref_preprocessing}

+ [Google](http://www.google.com)
+ [StackOverFlow](http://www.stackoverflow.com)
+ [Kaggle Kernel: Military VS Civilian Crashes](https://www.kaggle.com/adhok93/military-vs-civilian-crashes/code)
+ [Kaggle Kernel: Data Cleaning via Airplane Crashes](https://www.kaggle.com/danielviray/data-cleaning-via-airplane-crashes)

We understood how to differentiate whether an operator is Military institution or a Civilian operator from adhok93's kernel.
<br><br>
We got a lot of help from danielviray's kernel in order to accomplish data preprocessing, but we used a different approach and generated our own columns.

## EDA {#ref_eda}

+ [Word Cloud Fundamentals](http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know)




